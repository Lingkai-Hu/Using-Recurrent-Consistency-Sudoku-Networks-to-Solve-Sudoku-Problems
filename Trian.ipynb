{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from tensorflow.contrib.layers.python.layers import instance_norm\n",
    "import random\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load Dataset'''\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "question=np.load(r'dataset\\question.npy')\n",
    "answer=np.load(r'dataset\\answer.npy')\n",
    "answer=answer[np.sum(question>0,(1,2))<=20]\n",
    "question=question[np.sum(question>0,(1,2))<=20]\n",
    "\n",
    "'''Data Augmentation'''\n",
    "def shuffle(que,ans):\n",
    "    li=np.arange(0,9).reshape([3,3])\n",
    "    np.random.shuffle(li)\n",
    "    for _ in li:\n",
    "        np.random.shuffle(_)\n",
    "    li=li.reshape([-1])\n",
    "\n",
    "    il=np.arange(0,9).reshape([3,3])\n",
    "    np.random.shuffle(il)\n",
    "    for _ in il:\n",
    "        np.random.shuffle(_)\n",
    "    il=il.reshape([-1])\n",
    "    que=que[:,li,:][:,:,il]+10\n",
    "    ans=ans[:,li,:][:,:,il]+10\n",
    "    que[que==10]=0\n",
    "    numtra=np.arange(11,20)\n",
    "    np.random.shuffle(numtra)\n",
    "    for n,_ in enumerate(numtra):\n",
    "        que[que==_]=n+1\n",
    "        ans[ans==_]=n+1\n",
    "    if np.random.randint(0,2):\n",
    "        que=que.transpose(0,2,1)\n",
    "        ans=ans.transpose(0,2,1)\n",
    "    return que,ans\n",
    "\n",
    "num=0\n",
    "def next_batch(batch_size=128,num_data=180000):\n",
    "    global num\n",
    "    begin=num\n",
    "    end=num+batch_size\n",
    "    num=end\n",
    "    if(end>=num_data-1):\n",
    "        end=num_data-1\n",
    "        num=0\n",
    "    q,a=shuffle(question[begin:end],answer[begin:end])\n",
    "    q=np.reshape(q,[-1,9,9,1])==np.arange(0,10)\n",
    "    a=np.reshape(a,[-1,9,9,1])==np.arange(1,10)\n",
    "    return q.astype(np.float32),a.astype(np.float32)\n",
    "\n",
    "'''Normalization Function'''\n",
    "def bn(x):\n",
    "    x=instance_norm(x, epsilon=1e-05)\n",
    "    return x\n",
    "\n",
    "'''Linear Function'''\n",
    "def line(x):\n",
    "    return x\n",
    "\n",
    "'''conv'''\n",
    "def conv(x,filters,kernel_size=3,stride=1,activation=tf.nn.leaky_relu, padding='valid',norm=True,bias=False):\n",
    "    with tf.name_scope('conv'):\n",
    "        x=tf.layers.conv2d(x,filters,kernel_size=kernel_size, strides=stride, padding=padding,use_bias=bias)\n",
    "        if norm:\n",
    "            x=bn(x)\n",
    "            x=activation(x)\n",
    "    return x\n",
    "\n",
    "'''dconv'''\n",
    "def dconv(x,filters,kernel_size=3,stride=1,activation=tf.nn.leaky_relu,padding='valid',norm=True,bias=False):\n",
    "    with tf.name_scope('dconv'):\n",
    "        x=tf.layers.conv2d_transpose(x,filters,kernel_size=kernel_size, strides=stride, padding=padding,use_bias=bias)\n",
    "        if norm:\n",
    "            x=bn(x)\n",
    "            x=activation(x)\n",
    "    return x\n",
    "\n",
    "'''SDconv'''\n",
    "def sdconv(x,filters,activation=tf.nn.leaky_relu):\n",
    "    x1=conv(x,filters*2,kernel_size=[1,3],stride=(1,3),activation=activation)\n",
    "    x2=conv(x,filters*2,kernel_size=[3,1],stride=(3,1),activation=activation)\n",
    "    x3=x\n",
    "    x13=dconv(x1,filters*1,kernel_size=[1,1],activation=activation,stride=1)\n",
    "    x23=dconv(x2,filters*1,kernel_size=[1,1],activation=activation,stride=1)\n",
    "    \n",
    "    x11=conv(x1,filters*2,kernel_size=[1,3],stride=(1,3),activation=activation)\n",
    "    x12=conv(x1,filters*1,kernel_size=[3,1],stride=(3,1),activation=activation)\n",
    "    x21=conv(x2,filters*1,kernel_size=[1,3],stride=(1,3),activation=activation)\n",
    "    x22=conv(x2,filters*2,kernel_size=[3,1],stride=(3,1),activation=activation)\n",
    "    x12=tf.concat([x12,x21],-1)\n",
    "    x113=dconv(x11,filters*1,kernel_size=[1,1],activation=activation,stride=1)\n",
    "    x123=dconv(x12,filters*1,kernel_size=[1,1],activation=activation,stride=1)\n",
    "    x223=dconv(x22,filters*1,kernel_size=[1,1],activation=activation,stride=1)\n",
    "    \n",
    "    x112=conv(x11,filters*2,kernel_size=[3,1],stride=(3,1),activation=activation)\n",
    "    x121=conv(x12,filters*2,kernel_size=[1,3],stride=(1,3),activation=activation)\n",
    "    x212=conv(x12,filters*2,kernel_size=[3,1],stride=(3,1),activation=activation)\n",
    "    x221=conv(x22,filters*2,kernel_size=[1,3],stride=(1,3),activation=activation)\n",
    "    x121=tf.concat([x112,x121],-1)\n",
    "    x212=tf.concat([x212,x221],-1)\n",
    "    x1213=dconv(x121,filters*2,kernel_size=[1,1],activation=activation,stride=1)\n",
    "    x2123=dconv(x212,filters*2,kernel_size=[1,1],activation=activation,stride=1)\n",
    "    \n",
    "    x1212=conv(x121,filters*4,kernel_size=[3,1],stride=(3,1),activation=activation)\n",
    "    x2121=conv(x212,filters*4,kernel_size=[1,3],stride=(1,3),activation=activation)\n",
    "    x1212=tf.concat([x1212,x2121],-1)\n",
    "    \n",
    "    x121=dconv(x1212,filters*2,kernel_size=[3,1],stride=(3,1),activation=activation)\n",
    "    x212=dconv(x1212,filters*2,kernel_size=[1,3],stride=(1,3),activation=activation)\n",
    "    x121=tf.concat([x121,x1213],-1)\n",
    "    x212=tf.concat([x212,x2123],-1)\n",
    "    \n",
    "    x11=dconv(x121,filters*1,kernel_size=[3,1],stride=(3,1),activation=activation)\n",
    "    x12=dconv(x121,filters*1,kernel_size=[1,3],stride=(1,3),activation=activation)\n",
    "    x21=dconv(x212,filters*1,kernel_size=[3,1],stride=(3,1),activation=activation)\n",
    "    x22=dconv(x212,filters*1,kernel_size=[1,3],stride=(1,3),activation=activation)\n",
    "    x11=tf.concat([x11,x113],-1)\n",
    "    x12=tf.concat([x12,x21,x123],-1)\n",
    "    x22=tf.concat([x22,x223],-1)\n",
    "    \n",
    "    x1=dconv(x11,filters*1,kernel_size=[1,3],stride=(1,3),activation=activation)\n",
    "    x1_=dconv(x12,filters*1,kernel_size=[3,1],stride=(3,1),activation=activation)\n",
    "    x2=dconv(x12,filters*1,kernel_size=[1,3],stride=(1,3),activation=activation)\n",
    "    x2_=dconv(x22,filters*1,kernel_size=[3,1],stride=(3,1),activation=activation)\n",
    "    x1=tf.concat([x1,x1_,x13],-1)\n",
    "    x2=tf.concat([x2,x2_,x23],-1)\n",
    "    \n",
    "    x=dconv(x1,int(filters*1),kernel_size=[1,3],stride=(1,3),activation=line,norm=False)\n",
    "    x_=dconv(x2,int(filters*1),kernel_size=[3,1],stride=(3,1),activation=line,norm=False)\n",
    "\n",
    "    x=tf.concat([x3,x,x_],-1)\n",
    "    x=activation(bn(x))\n",
    "    x=dconv(x,int(filters*1),kernel_size=[1,1],activation=line,stride=1,norm=False)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sdconv0(net):\n",
    "    reuse=len([t for t in tf.global_variables() if t.name.startswith('step0')])>0\n",
    "    with tf.variable_scope('step0',reuse=reuse):\n",
    "        net=sdconv(net,256)\n",
    "    return net\n",
    "\n",
    "def sdconv1(net):\n",
    "    reuse=len([t for t in tf.global_variables() if t.name.startswith('step1')])>0\n",
    "    with tf.variable_scope('step1',reuse=reuse):\n",
    "        net=sdconv(net,256)\n",
    "    return net\n",
    "\n",
    "def sdconv2(net):\n",
    "    reuse=len([t for t in tf.global_variables() if t.name.startswith('step2')])>0\n",
    "    with tf.variable_scope('step2',reuse=reuse):\n",
    "        net=sdconv(net,256)\n",
    "    return net\n",
    "\n",
    "def sdconv3(net):\n",
    "    reuse=len([t for t in tf.global_variables() if t.name.startswith('step3')])>0\n",
    "    with tf.variable_scope('step3',reuse=reuse):\n",
    "        net=sdconv(net,256)\n",
    "    return net\n",
    "\n",
    "def sdconv4(net):\n",
    "    reuse=len([t for t in tf.global_variables() if t.name.startswith('step4')])>0\n",
    "    with tf.variable_scope('step4',reuse=reuse):\n",
    "        net=sdconv(net,256)\n",
    "    return net\n",
    "\n",
    "def sdconv5(net):\n",
    "    reuse=len([t for t in tf.global_variables() if t.name.startswith('step5')])>0\n",
    "    with tf.variable_scope('step5',reuse=reuse):\n",
    "        net=sdconv(net,256)\n",
    "    return net\n",
    "\n",
    "\n",
    "def sdconv6(net):\n",
    "    reuse=len([t for t in tf.global_variables() if t.name.startswith('step6')])>0\n",
    "    with tf.variable_scope('step6',reuse=reuse):\n",
    "        net=sdconv(net,256)\n",
    "    return net\n",
    "\n",
    "def sdconv7(net):\n",
    "    reuse=len([t for t in tf.global_variables() if t.name.startswith('step7')])>0\n",
    "    with tf.variable_scope('step7',reuse=reuse):\n",
    "        net=sdconv(net,256)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.placeholder(tf.float32,[None,9,9,10])\n",
    "y=tf.placeholder(tf.float32,[None,9,9,9])\n",
    "net0=dconv(x,256,kernel_size=[1,1],activation=line,stride=1,norm=False)\n",
    "\n",
    "net1=sdconv0(net0)\n",
    "net=net1[:,:,:,-9:]\n",
    "one=tf.square((tf.minimum(1.0,net)-1)*y)\n",
    "zero=tf.square(tf.maximum(0.0,net)*(1-y))\n",
    "loss10 = tf.reduce_mean(one+zero)\n",
    "argmax=tf.argmax(net, -1)\n",
    "correct_prediction=tf.cast(tf.equal(argmax, tf.argmax(y, -1)), tf.float32)\n",
    "accuracy10 = tf.reduce_mean(correct_prediction)\n",
    "\n",
    "net1=sdconv0(net1)\n",
    "net=net1[:,:,:,-9:]\n",
    "one=tf.square((tf.minimum(1.0,net)-1)*y)\n",
    "zero=tf.square(tf.maximum(0.0,net)*(1-y))\n",
    "loss11 = tf.reduce_mean(one+zero)\n",
    "argmax=tf.argmax(net, -1)\n",
    "correct_prediction=tf.cast(tf.equal(argmax, tf.argmax(y, -1)), tf.float32)\n",
    "accuracy11 = tf.reduce_mean(correct_prediction)\n",
    "\n",
    "net1=sdconv0(net1)\n",
    "net=net1[:,:,:,-9:]\n",
    "one=tf.square((tf.minimum(1.0,net)-1)*y)\n",
    "zero=tf.square(tf.maximum(0.0,net)*(1-y))\n",
    "loss12 = tf.reduce_mean(one+zero)\n",
    "argmax=tf.argmax(net, -1)\n",
    "correct_prediction=tf.cast(tf.equal(argmax, tf.argmax(y, -1)), tf.float32)\n",
    "accuracy12 = tf.reduce_mean(correct_prediction)\n",
    "\n",
    "net1=sdconv0(net1)\n",
    "net=net1[:,:,:,-9:]\n",
    "one=tf.square((tf.minimum(1.0,net)-1)*y)\n",
    "zero=tf.square(tf.maximum(0.0,net)*(1-y))\n",
    "loss13 = tf.reduce_mean(one+zero)\n",
    "argmax=tf.argmax(net, -1)\n",
    "correct_prediction=tf.cast(tf.equal(argmax, tf.argmax(y, -1)), tf.float32)\n",
    "accuracy13 = tf.reduce_mean(correct_prediction)\n",
    "\n",
    "for i in range(1,8):\n",
    "    exec('net{} = sdconv{}(net{})'.format(i+1,i,i))\n",
    "    for _ in range(3):\n",
    "        exec('net{} = sdconv{}(net{})'.format(i+1,i,i+1))\n",
    "\n",
    "for i in range(2,9):\n",
    "    exec('net = net{}[:,:,:,-9:]'.format(i))\n",
    "    one=tf.square((tf.minimum(1.0,net)-1)*y)\n",
    "    zero=tf.square(tf.maximum(0.0,net)*(1-y))\n",
    "    exec('loss{} = tf.reduce_mean(one+zero)'.format(i))\n",
    "    argmax=tf.argmax(net, -1)\n",
    "    correct_prediction=tf.cast(tf.equal(argmax, tf.argmax(y, -1)), tf.float32)\n",
    "    exec('accuracy{} = tf.reduce_mean(correct_prediction)'.format(i))\n",
    "\n",
    "'''Visualization Module'''\n",
    "acchold=tf.Variable(0.0,trainable=False)\n",
    "losshold=tf.Variable(0.0,trainable=False)\n",
    "tf.summary.scalar('accuracy',acchold)\n",
    "tf.summary.scalar('loss',losshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate=f.Variable(0.0001,trainable=False)\n",
    "'''Pre Training'''\n",
    "for i in range(4):\n",
    "    exec('train1{}=tf.train.AdamOptimizer(rate).minimize(loss1{})'.format(i,i))\n",
    "'''Following Training'''\n",
    "for i in range(2,9):\n",
    "    exec('train{}=tf.train.AdamOptimizer(rate).minimize(loss{},var_list=var)'.format(i,i))\n",
    "\n",
    "sess=tf.Session()\n",
    "merged=tf.summary.merge_all()\n",
    "writer=tf.summary.FileWriter(r\"board/train\")\n",
    "ac,cr=0,0\n",
    "number=0\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# saver=tf.train.Saver(tf.trainable_variables())\n",
    "# saver.restore(sess,r'save\\log')\n",
    "tf.train.start_queue_runners(sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Pre Training'''\n",
    "ra=0.0128\n",
    "show=100\n",
    "for step in range(4):\n",
    "    for i in range(10000):\n",
    "        if i%show==(show-1):\n",
    "            result=sess.run(merged,feed_dict={x:feed,y:label,acchold:ac,losshold:cr})\n",
    "            writer.add_summary(result,number)\n",
    "            ac,cr=0,0\n",
    "\n",
    "        feed,label=next_batch(128,num_data=40000)\n",
    "        feed_dict={x:feed,y:label,rate:ra}\n",
    "        exec('_,_ac,_cr=sess.run([train1{},accuracy1{},loss1{}],feed_dict=feed_dict)'.format(step,step,step))\n",
    "        ac+=np.array(_ac)/show\n",
    "        cr+=np.array(_cr)/show\n",
    "        number+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Following Training'''\n",
    "for step in range(2,9):\n",
    "    var1=[_ for _ in tf.trainable_variables() if 'step{}'.format(step-2) in _.name]\n",
    "    var2=[_ for _ in tf.trainable_variables() if 'step{}'.format(step-1) in _.name]\n",
    "    a=[tf.assign(var2[i], var1[i]) for i in range(len(var1))]\n",
    "    b=sess.run(a)\n",
    "    for i in range(10000):\n",
    "        if i%show==(show-1):\n",
    "            result=sess.run(merged,feed_dict={x:feed,y:label,acchold:ac,losshold:cr})\n",
    "            writer.add_summary(result,number)\n",
    "            ac,cr=0,0\n",
    "\n",
    "        feed,label=next_batch(128,num_data=40000)\n",
    "        feed_dict={x:feed,y:label,rate:ra}\n",
    "        exec('_,_ac,_cr=sess.run([train{},accuracy{},loss{}],feed_dict=feed_dict)'.format(step,step,step))\n",
    "        ac+=np.array(_ac)/show\n",
    "        cr+=np.array(_cr)/show\n",
    "        number+=1\n",
    "        ra*=0.99993068768415357191320015590215\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver=tf.train.Saver()\n",
    "save_path=saver.save(sess,r'save\\log')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
